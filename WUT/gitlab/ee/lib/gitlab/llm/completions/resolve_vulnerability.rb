# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ResolveVulnerability < Gitlab::Llm::Completions::Base
        include Gitlab::Llm::Completions::ResolveVulnerability::Helpers
        include Gitlab::InternalEventsTracking

        EmptyResponseError = Class.new(StandardError)

        def execute
          ai_response, diff_extracted, description_options = response_for(user, vulnerability, @options)

          vulnerable_merge_request_id = merge_request_gid(@options[:vulnerable_merge_request_id])
          response = if diff_extracted
                       create_merge_request_with_resolution(
                         user,
                         vulnerability,
                         ai_response,
                         description_options,
                         vulnerable_merge_request_id: vulnerable_merge_request_id
                       )

                       # TODO: I think the note creation logic can be
                       # pulled all the way to this level
                       #
                       # Something like:
                       #
                       #    attach_note(mr) if vulnerable_merge_request_id
                     else
                       ai_response
                     end

          response_modifier = modify_response(response)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        rescue Templates::Vulnerabilities::CodeLengthError => error
          handle_warn(error, 'error_building_request')
        rescue EmptyResponseError => error
          handle_warn(error, 'error_response_received')
        rescue StandardError => error
          handle_error(error)
        end

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(user, vulnerability), expires_in: 10.minutes, skip_nil: true) do
            prompt = ai_prompt_class.new(vulnerability, options).to_prompt
            ai_response = request(user, prompt)

            extract_llm_change(ai_response)
          end
        end

        private

        def parse_ai_response(ai_response)
          raise EmptyResponseError, "Empty response from LLM" if ai_response.blank?

          ai_response.dig('predictions', 0, 'content') || ai_response.dig('content', 0, 'text') || ''
        end

        def error_response(error)
          response = formatted_error_response(error_message(error))
          response_modifier = modify_response(response)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        end

        def modify_response(response)
          ::Gitlab::Llm::ResponseModifiers::ResolveVulnerability.new(response)
        end
      end
    end
  end
end
