# frozen_string_literal: true

module Gitlab
  module Llm
    class PromptVersions
      class << self
        # Defines the major version of a prompt to be used. Prompts can have their minor and patch versions updated
        # independently, but this will guarantee that older versions of GitLab clients are still able to fetch
        # compatible prompts.
        # During development, new version testing should be guarded behind a feature flag.
        VERSIONS = {
          "categorize_question/base": "^1.0.0",
          "chat/build_reader/amazon_q": "^1.0.0",
          "chat/build_reader/base": "^1.0.0",
          "chat/build_reader/claude_3": "^1.0.0",
          "chat/build_reader/mistral": "^1.0.0",
          "chat/build_reader/mixtral": "^1.0.0",
          "chat/commit_reader/amazon_q": "^1.0.0",
          "chat/commit_reader/base": "^1.0.0",
          "chat/commit_reader/claude_3": "^1.0.0",
          "chat/commit_reader/gpt": "^1.0.0",
          "chat/commit_reader/mistral": "^1.0.0",
          "chat/commit_reader/mixtral": "^1.0.0",
          "chat/documentation_search/amazon_q": "^1.0.0",
          "chat/documentation_search/base": "^1.0.0",
          "chat/documentation_search/claude_3": "^1.0.0",
          "chat/documentation_search/gpt": "^1.0.0",
          "chat/documentation_search/mistral": "^1.0.0",
          "chat/documentation_search/mixtral": "^1.0.0",
          "chat/epic_reader/amazon_q": "^1.0.0",
          "chat/epic_reader/base": "^1.0.0",
          "chat/epic_reader/claude_3": "^1.0.0",
          "chat/epic_reader/gpt": "^1.0.0",
          "chat/epic_reader/mistral": "^1.0.0",
          "chat/epic_reader/mixtral": "^1.0.0",
          "chat/explain_code/amazon_q": "^1.0.0",
          "chat/explain_code/base": "^1.0.0",
          "chat/explain_code/claude_3": "^1.0.0",
          "chat/explain_code/gpt": "^1.0.0",
          "chat/explain_code/mistral": "^1.0.0",
          "chat/explain_code/mistral_nemo": "^1.0.0",
          "chat/explain_code/mixtral": "^1.0.0",
          "chat/explain_vulnerability/amazon_q": "^1.0.0",
          "chat/explain_vulnerability/base": "^1.0.0",
          "chat/explain_vulnerability/claude_3": "^1.0.0",
          "chat/explain_vulnerability/mistral": "^1.0.0",
          "chat/explain_vulnerability/mixtral": "^1.0.0",
          "chat/explain_vulnerability/gpt": "^1.0.0",
          "chat/explain_vulnerability/llama3": "^1.0.0",
          "chat/fix_code/amazon_q": "^1.0.0",
          "chat/fix_code/base": "^1.0.0",
          "chat/fix_code/claude_3": "^1.0.0",
          "chat/fix_code/gpt": "^1.0.0",
          "chat/fix_code/mistral": "^1.0.0",
          "chat/fix_code/mixtral": "^1.0.0",
          "chat/issue_reader/amazon_q": "^1.0.0",
          "chat/issue_reader/base": "^1.0.0",
          "chat/issue_reader/claude_3": "^1.0.0",
          "chat/issue_reader/gpt": "^1.0.0",
          "chat/issue_reader/mistral": "^1.0.0",
          "chat/issue_reader/mixtral": "^1.0.0",
          "chat/merge_request_reader/amazon_q": "^1.0.0",
          "chat/merge_request_reader/base": "^1.0.0",
          "chat/merge_request_reader/claude_3": "^1.0.0",
          "chat/merge_request_reader/mistral": "^1.0.0",
          "chat/merge_request_reader/mixtral": "^1.0.0",
          "chat/react/amazon_q": "^1.0.0",
          "chat/react/base": "^1.0.0",
          "chat/react/claude_3": "^1.0.0",
          "chat/react/gpt": "^1.0.0",
          "chat/react/mistral": "^1.0.0",
          "chat/react/mistral_nemo": "^1.0.0",
          "chat/react/mixtral": "^1.0.0",
          "chat/react/vertex": "^1.0.0",
          "chat/refactor_code/amazon_q": "^1.0.0",
          "chat/refactor_code/base": "^1.0.0",
          "chat/refactor_code/claude_3": "^1.0.0",
          "chat/refactor_code/gpt": "^1.0.0",
          "chat/refactor_code/mistral": "^1.0.0",
          "chat/refactor_code/mixtral": "^1.0.0",
          "chat/summarize_comments/amazon_q": "^1.0.0",
          "chat/summarize_comments/base": "^1.0.0",
          "chat/summarize_comments/claude_3": "^1.0.0",
          "chat/summarize_comments/mistral": "^1.0.0",
          "chat/summarize_comments/mixtral": "^1.0.0",
          "chat/troubleshoot_job/amazon_q": "^1.0.0",
          "chat/troubleshoot_job/base": "^1.0.0",
          "chat/troubleshoot_job/claude_3": "^1.0.0",
          "chat/troubleshoot_job/mistral": "^1.0.0",
          "chat/troubleshoot_job/mixtral": "^1.0.0",
          "chat/troubleshoot_job/gpt": "^1.0.0",
          "chat/troubleshoot_job/llama3": "^1.0.0",
          "chat/write_tests/amazon_q": "^1.0.0",
          "chat/write_tests/base": "^1.0.0",
          "chat/write_tests/claude_3": "^1.0.0",
          "chat/write_tests/gpt": "^1.0.0",
          "chat/write_tests/mistral": "^1.0.0",
          "chat/write_tests/mixtral": "^1.0.0",
          "code_suggestions/completions/claude_3": "^1.0.0",
          "code_suggestions/completions/codegemma": "^1.0.0",
          "code_suggestions/completions/codellama": "^1.0.0",
          "code_suggestions/completions/codestral": "^1.0.0",
          "code_suggestions/completions/deepseekcoder": "^1.0.0",
          "code_suggestions/completions/gpt": "^1.0.0",
          "code_suggestions/completions/llama3": "^1.0.0",
          "code_suggestions/completions/mistral": "^1.0.0",
          "code_suggestions/completions/mixtral": "^1.0.0",
          "code_suggestions/generations/base": "^2.0.1",
          "code_suggestions/generations/claude_3": "^1.0.0",
          "code_suggestions/generations/codegemma": "^1.0.0",
          "code_suggestions/generations/codellama": "^1.0.0",
          "code_suggestions/generations/codestral": "^1.0.0",
          "code_suggestions/generations/deepseekcoder": "^1.0.0",
          "code_suggestions/generations/gpt": "^1.0.0",
          "code_suggestions/generations/llama3": "^1.0.0",
          "code_suggestions/generations/mistral": "^1.0.0",
          "code_suggestions/generations/mixtral": "^1.0.0",
          "generate_commit_message/base": "^1.0.0",
          "generate_description/base": "^1.0.0",
          "generate_description/claude_3": "^1.0.0",
          "glab_ask_git_command/base": "^1.0.0",
          "glab_ask_git_command/claude_3": "^1.0.0",
          "glab_ask_git_command/gpt": "^1.0.0",
          "glab_ask_git_command/mistral": "^1.0.0",
          "glab_ask_git_command/mixtral": "^1.0.0",
          "glab_ask_git_command/llama3": "^1.0.0",
          "measure_comment_temperature/base": "^1.0.0",
          "model_configuration/check/claude_3": "^1.0.0",
          "model_configuration/check/codegemma": "^1.0.0",
          "model_configuration/check/codellama": "^1.0.0",
          "model_configuration/check/codestral": "^1.0.0",
          "model_configuration/check/deepseekcoder": "^1.0.0",
          "model_configuration/check/gpt": "^1.0.0",
          "model_configuration/check/llama3": "^1.0.0",
          "model_configuration/check/mistral": "^1.0.0",
          "model_configuration/check/mistral_nemo": "^1.0.0",
          "model_configuration/check/mixtral": "^1.0.0",
          "resolve_vulnerability/base": "^1.0.0",
          "resolve_vulnerability/amazon_q": "^1.0.0",
          "resolve_vulnerability/claude_3": "^1.0.0",
          "resolve_vulnerability/gpt": "^1.0.0",
          "resolve_vulnerability/mistral": "^1.0.0",
          "resolve_vulnerability/mixtral": "^1.0.0",
          "resolve_vulnerability/llama3": "^1.0.0",
          "review_merge_request/base": "^1.0.0",
          "summarize_new_merge_request/base": "^2.0.0",
          "summarize_new_merge_request/amazon_q": "^1.0.0",
          "summarize_new_merge_request/claude_3": "^1.0.0",
          "summarize_new_merge_request/mistral": "^1.0.0",
          "summarize_new_merge_request/mixtral": "^1.0.0",
          "summarize_new_merge_request/gpt": "^1.0.0",
          "summarize_new_merge_request/llama3": "^1.0.0",
          "summarize_review/base": "^2.0.0",
          "summarize_review/amazon_q": "^1.0.0",
          "summarize_review/claude_3": "^1.0.0",
          "summarize_review/mistral": "^1.0.0",
          "summarize_review/mixtral": "^1.0.0",
          "summarize_review/gpt": "^1.0.0",
          "summarize_review/llama3": "^1.0.0"
        }.freeze

        def version_for_prompt(prompt_name, model_family)
          full_name = if model_family.nil?
                        "#{prompt_name}/base"
                      else
                        "#{prompt_name}/#{model_family}"
                      end

          VERSIONS[full_name.to_sym] || "^1.0.0"
        end
      end
    end
  end
end
